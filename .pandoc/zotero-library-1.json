[{"id":"8644402","author":[{"family":"Guerrini","given":"Stefano"},{"family":"Chiani","given":"Marco"},{"family":"Conti","given":"Andrea"}],"citation-key":"8644402","container-title":"2018 IEEE globecom workshops (GC wkshps)","DOI":"10.1109/GLOCOMW.2018.8644402","issued":{"date-parts":[["2018"]]},"page":"1-5","title":"Secure key throughput of intermittent trusted-relay QKD protocols","type":"paper-conference"},{"id":"aghajanyanIntrinsicDimensionalityExplains2020","abstract":"Although pretrained language models can be ﬁne-tuned to produce state-of-theart results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing ﬁne-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for ﬁne-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a ﬁxed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Aghajanyan","given":"Armen"},{"family":"Zettlemoyer","given":"Luke"},{"family":"Gupta","given":"Sonal"}],"citation-key":"aghajanyanIntrinsicDimensionalityExplains2020","issued":{"date-parts":[["2020",12,22]]},"language":"en","number":"arXiv:2012.13255","publisher":"arXiv","source":"arXiv.org","title":"Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning","type":"article","URL":"http://arxiv.org/abs/2012.13255"},{"id":"aguadomartinEnablingQuantumCryptography","author":[{"family":"Aguado Martín","given":"Alejandro"}],"citation-key":"aguadomartinEnablingQuantumCryptography","language":"en","source":"Zotero","title":"Enabling Quantum Cryptography in Novel Network Paradigms","type":"thesis"},{"id":"angelisArtificialIntelligencePhysical2023","abstract":"Symbolic regression (SR) is a machine learning-based regression method based on genetic programming principles that integrates techniques and processes from heterogeneous scientific fields and is capable of providing analytical equations purely from data. This remarkable characteristic diminishes the need to incorporate prior knowledge about the investigated system. SR can spot profound and elucidate ambiguous relations that can be generalizable, applicable, explainable and span over most scientific, technological, economical, and social principles. In this review, current state of the art is documented, technical and physical characteristics of SR are presented, the available programming techniques are investigated, fields of application are explored, and future perspectives are discussed.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Angelis","given":"Dimitrios"},{"family":"Sofos","given":"Filippos"},{"family":"Karakasidis","given":"Theodoros E."}],"citation-key":"angelisArtificialIntelligencePhysical2023","container-title":"Archives of Computational Methods in Engineering","container-title-short":"Arch Computat Methods Eng","DOI":"10.1007/s11831-023-09922-z","ISSN":"1134-3060, 1886-1784","issue":"6","issued":{"date-parts":[["2023",7]]},"language":"en","page":"3845-3865","source":"DOI.org (Crossref)","title":"Artificial Intelligence in Physical Sciences: Symbolic Regression Trends and Perspectives","title-short":"Artificial Intelligence in Physical Sciences","type":"article-journal","URL":"https://link.springer.com/10.1007/s11831-023-09922-z","volume":"30"},{"id":"bassiQuantumKeyDistribution2023","abstract":"Quantum Key Distribution (QKD) is a mechanism that allows two entities to agree on a secret key over a quantum channel. Recently, commercially viable QKD technology appeared on the market and the European Telecommunications Standards Institute (ETSI) established a working group to standardize QKD for commercial applications. Several testbeds and pre-commercial QKD networks are being deployed in the world, all requiring sophisticated control mechanisms to enable key exchanges between non-adjacent nodes for remote applications. In this paper, we implement and evaluate a prototype that realizes the architecture defined by ETSI and uses trusted relay nodes to exchange keys between non-adjacent nodes. In particular, we present the Software-Defined Network (SDN) Controller and the Software-Defined QKD nodes designed to control the QKD experimental testbed in Milan, named PoliQI. We test the proposed prototype in an emulation environment, showing its advantages in inter-datacenter communication scenarios in terms of key delivery time and application acceptance ratio.","accessed":{"date-parts":[["2024",5,25]]},"author":[{"family":"Bassi","given":"Riccardo"},{"family":"Zhang","given":"Qiaolun"},{"family":"Gatto","given":"Alberto"},{"family":"Tornatore","given":"Massimo"},{"family":"Verticale","given":"Giacomo"}],"citation-key":"bassiQuantumKeyDistribution2023","container-title":"2023 19th International Conference on the Design of Reliable Communication Networks (DRCN)","DOI":"10.1109/DRCN57075.2023.10108347","event-place":"Vilanova i la Geltru, Spain","event-title":"2023 19th International Conference on the Design of Reliable Communication Networks (DRCN)","ISBN":"978-1-66547-598-3","issued":{"date-parts":[["2023",4,17]]},"language":"en","license":"https://doi.org/10.15223/policy-029","page":"1-7","publisher":"IEEE","publisher-place":"Vilanova i la Geltru, Spain","source":"DOI.org (Crossref)","title":"Quantum Key Distribution with Trusted Relay using an ETSI-compliant Software-Defined Controller","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10108347/"},{"id":"bellomariniVadalogSystemDatalogbased2018","abstract":"Over the past years, there has been a resurgence of Datalog-based systems in the database community as well as in industry. In this context, it has been recognized that to handle the complex knowledge-based scenarios encountered today, such as reasoning over large knowledge graphs, Datalog has to be extended with features such as existential quantiﬁcation. Yet, Datalog-based reasoning in the presence of existential quantiﬁcation is in general undecidable. Many eﬀorts have been made to deﬁne decidable fragments. Warded Datalog+/- is a very promising one, as it captures PTIME complexity while allowing ontological reasoning. Yet so far, no implementation of Warded Datalog+/- was available. In this paper we present the Vadalog system, a Datalog-based system for performing complex logic reasoning tasks, such as those required in advanced knowledge graphs. The Vadalog system is Oxford’s contribution to the VADA research programme, a joint eﬀort of the universities of Oxford, Manchester and Edinburgh and around 20 industrial partners. As the main contribution of this paper, we illustrate the ﬁrst implementation of Warded Datalog+/-, a high-performance Datalog+/- system utilizing an aggressive termination control strategy. We also provide a comprehensive experimental evaluation.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Bellomarini","given":"Luigi"},{"family":"Gottlob","given":"Georg"},{"family":"Sallinger","given":"Emanuel"}],"citation-key":"bellomariniVadalogSystemDatalogbased2018","issued":{"date-parts":[["2018",7,23]]},"language":"en","number":"arXiv:1807.08709","publisher":"arXiv","source":"arXiv.org","title":"The Vadalog System: Datalog-based Reasoning for Knowledge Graphs","title-short":"The Vadalog System","type":"article","URL":"http://arxiv.org/abs/1807.08709"},{"id":"branciardDetectionLoopholeBell2011","abstract":"A common problem in Bell-type experiments is the well-known detection loophole: if\nthe detection efficiencies are not perfect and if one simply postselects the conclusive\nevents, one might observe a violation of a Bell inequality, even though a local model\ncould have explained the experimental results In this paper, we analyze the set of\nall postselected correlations that can be explained by a local model, and show that\nit forms a polytope, larger than the Bell local polytope We characterize the facets\nof this postselected local polytope in the Clauser-Horne-Shimony-Holt scenario, where\ntwo parties have binary inputs and outcomes Our approach gives interesting insights\non the detection loophole problem","author":[{"family":"Branciard","given":"Cyril"}],"citation-key":"branciardDetectionLoopholeBell2011","container-title":"Physical Review A","event-place":"United States","ISSN":"1050-2947","issue":"3","issued":{"date-parts":[["2011"]]},"page":"032123-0321238","publisher-place":"United States","title":"Detection loophole in Bell experiments: How postselection modifies the requirements to observe nonlocality","type":"article-journal","URL":"http://inis.iaea.org/search/search.aspx?orig_q=RN:43016405","volume":"83"},{"id":"brask2022gaussian","author":[{"family":"Brask","given":"Jonatan Bohr"}],"citation-key":"brask2022gaussian","issued":{"date-parts":[["2022"]]},"title":"Gaussian states and operations – a quick reference","type":"chapter"},{"id":"bronstein2021geometric","author":[{"family":"Bronstein","given":"Michael M."},{"family":"Bruna","given":"Joan"},{"family":"Cohen","given":"Taco"},{"family":"Veličković","given":"Petar"}],"citation-key":"bronstein2021geometric","issued":{"date-parts":[["2021"]]},"title":"Geometric deep learning: Grids, groups, graphs, geodesics, and gauges","type":"book"},{"id":"brumfielQuantumUncertaintyNot2012","abstract":"A common interpretation of Heisenberg's uncertainty principle is proven false.","author":[{"family":"Brumfiel","given":"Geoff"}],"citation-key":"brumfielQuantumUncertaintyNot2012","container-title":"Nature","container-title-short":"Nature","DOI":"10.1038/nature.2012.11394","ISSN":"1476-4687","issued":{"date-parts":[["2012",9,11]]},"title":"Quantum uncertainty not all in the measurement","type":"article-journal","URL":"https://doi.org/10.1038/nature.2012.11394"},{"id":"cederlofAuthenticationQuantumKey","abstract":"Quantum key growing, often called quantum cryptography or quantum key distribution, is a method using some properties of quantum mechanics to create a secret shared cryptography key even if an eavesdropper has access to unlimited computational power. A vital but often neglected part of the method is unconditionally secure message authentication. This thesis examines the security aspects of authentication in quantum key growing. Important concepts are formalized as Python program source code, a comparison between quantum key growing and a classical system using trusted couriers is included, and the chain rule of entropy is generalized to any Re´nyi entropy. Finally and most importantly, a security ﬂaw is identiﬁed which makes the probability to eavesdrop on the system undetected approach unity as the system is in use for a long time, and a solution to this problem is provided.","author":[{"family":"Cederlof","given":"Jorgen"}],"citation-key":"cederlofAuthenticationQuantumKey","language":"en","source":"Zotero","title":"Authentication in quantum key growing","type":"thesis"},{"id":"cederlofSecurityAspectsAuthentication2008","abstract":"Unconditionally secure message authentication is an important part of quantum cryptography (QC). In this correspondence, we analyze security effects of using a key obtained from QC for authentication purposes in later rounds of QC. In particular, the eavesdropper gains partial knowledge on the key in QC that may have an effect on the security of the authentication in the later round. Our initial analysis indicates that this partial knowledge has little effect on the authentication part of the system, in agreement with previous results on the issue. However, when taking the full QC protocol into account, the picture is different. By accessing the quantum channel used in QC, the attacker can change the message to be authenticated. This, together with partial knowledge of the key, does incur a security weakness of the authentication. The underlying reason for this is that the authentication used, which is insensitive to such message changes when the key is unknown, becomes sensitive when used with a partially known key. We suggest a simple solution to this problem, and stress usage of this or an equivalent extra security measure in QC.","accessed":{"date-parts":[["2024",5,20]]},"author":[{"family":"Cederlof","given":"JÖrgen"},{"family":"Larsson","given":"Jan-Åke"}],"citation-key":"cederlofSecurityAspectsAuthentication2008","container-title":"IEEE Transactions on Information Theory","container-title-short":"IEEE Trans. Inform. Theory","DOI":"10.1109/TIT.2008.917697","ISSN":"0018-9448","issue":"4","issued":{"date-parts":[["2008",4]]},"language":"en","license":"https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html","page":"1735-1741","source":"DOI.org (Crossref)","title":"Security Aspects of the Authentication Used in Quantum Cryptography","type":"article-journal","URL":"http://ieeexplore.ieee.org/document/4475356/","volume":"54"},{"id":"chenGroupTheoryRubik","author":[{"family":"Chen","given":"Janet"}],"citation-key":"chenGroupTheoryRubik","language":"en","source":"Zotero","title":"Group Theory and the Rubik's Cube","type":"article-journal"},{"id":"collinsQuantumRelaysLong2005","abstract":"Quantum Cryptography is on the verge of commercial application. One of its greatest limitations is over long distance - secret key rates are low and the longest fibre over which any key has been exchanged is currently 100 km. We investigate the quantum relay, which can increase the maximum distance at which quantum cryptography is possible. The relay splits the channel into sections, and sends a different photon across each section, increasing the signal to noise ratio. The photons are linked as in teleportation, with entangled photon pairs and Bell measurements. We show that such a scheme could allow cryptography over hundreds of kilometers with today's detectors. It could not, however, improve the rate of key exchange over distances where the standard single section scheme already works. We also show that reverse key reconciliation, previously used in continuous variable quantum cryptography, gives a secure key over longer distances than forward key reconciliation.","accessed":{"date-parts":[["2024",5,20]]},"author":[{"family":"Collins","given":"Daniel"},{"family":"Gisin","given":"Nicolas"},{"family":"Riedmatten","given":"Hugues","non-dropping-particle":"de"}],"citation-key":"collinsQuantumRelaysLong2005","container-title":"Journal of Modern Optics","container-title-short":"Journal of Modern Optics","DOI":"10.1080/09500340412331283633","ISSN":"0950-0340, 1362-3044","issue":"5","issued":{"date-parts":[["2005",3,20]]},"language":"en","page":"735-753","source":"arXiv.org","title":"Quantum Relays for Long Distance Quantum Cryptography","type":"article-journal","URL":"http://arxiv.org/abs/quant-ph/0311101","volume":"52"},{"id":"cranmerInterpretableMachineLearning2023","abstract":"PySR is an open-source library for practical symbolic regression, a type of machine learning which aims to discover human-interpretable symbolic models. PySR was developed to democratize and popularize symbolic regression for the sciences, and is built on a high-performance distributed back-end, a flexible search algorithm, and interfaces with several deep learning packages. PySR's internal search algorithm is a multi-population evolutionary algorithm, which consists of a unique evolve-simplify-optimize loop, designed for optimization of unknown scalar constants in newly-discovered empirical expressions. PySR's backend is the extremely optimized Julia library SymbolicRegression.jl, which can be used directly from Julia. It is capable of fusing user-defined operators into SIMD kernels at runtime, performing automatic differentiation, and distributing populations of expressions to thousands of cores across a cluster. In describing this software, we also introduce a new benchmark, \"EmpiricalBench,\" to quantify the applicability of symbolic regression algorithms in science. This benchmark measures recovery of historical empirical equations from original and synthetic datasets.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Cranmer","given":"Miles"}],"citation-key":"cranmerInterpretableMachineLearning2023","issued":{"date-parts":[["2023",5,5]]},"language":"en","number":"arXiv:2305.01582","publisher":"arXiv","source":"arXiv.org","title":"Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl","type":"article","URL":"http://arxiv.org/abs/2305.01582"},{"id":"dettmersQLoRAEfficientFinetuning2023","abstract":"We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Dettmers","given":"Tim"},{"family":"Pagnoni","given":"Artidoro"},{"family":"Holtzman","given":"Ari"},{"family":"Zettlemoyer","given":"Luke"}],"citation-key":"dettmersQLoRAEfficientFinetuning2023","issued":{"date-parts":[["2023",5,23]]},"language":"en","number":"arXiv:2305.14314","publisher":"arXiv","source":"arXiv.org","title":"QLoRA: Efficient Finetuning of Quantized LLMs","title-short":"QLoRA","type":"article","URL":"http://arxiv.org/abs/2305.14314"},{"id":"ETSIGSQKD","abstract":"Quantum Key Distribution (QKD); Common Criteria Protection Profile - Pair of Prepare and Measure Quantum Key Distribution Modules","authority":"ETSI","citation-key":"ETSIGSQKD","section":"ISG QKD","title":"ETSI GS QKD 016","type":"standard"},{"id":"ETSIGSQKDa","abstract":"Quantum Key Distribution (QKD); Control Interface for Software Defined Networks","authority":"ETSI","citation-key":"ETSIGSQKDa","section":"ISG QKD","title":"ETSI GS QKD 015","type":"standard"},{"id":"evansTrustedNodeQKD2021","author":[{"family":"Evans","given":"Philip G."},{"family":"Alshowkan","given":"Muneer"},{"family":"Earl","given":"Duncan"},{"family":"Mulkey","given":"Daniel D."},{"family":"Newell","given":"Raymond"},{"family":"Peterson","given":"Glen"},{"family":"Safi","given":"Claira"},{"family":"Tripp","given":"Justin L."},{"family":"Peters","given":"Nicholas A."}],"citation-key":"evansTrustedNodeQKD2021","container-title":"IEEE Access","DOI":"10.1109/ACCESS.2021.3070222","issued":{"date-parts":[["2021"]]},"page":"105220-105229","title":"Trusted Node QKD at an Electrical Utility","type":"article-journal","volume":"9"},{"id":"fandinnoAnswerSetProgramming2021","abstract":"We take up an idea from the folklore of Answer Set Programming (ASP), namely that choices, integrity constraints along with a restricted rule format is sufﬁcient for ASP. We elaborate upon the foundations of this idea in the context of the logic of Here-and-There and show how it can be derived from the logical principle of extension by deﬁnition. We then provide an austere form of logic programs that may serve as a normalform for logic programs similar to conjunctive normalform in classical logic. Finally, we take the key ideas and propose a modeling methodology for ASP beginners and illustrate how it can be used.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Fandinno","given":"Jorge"},{"family":"Mishra","given":"Seemran"},{"family":"Romero","given":"Javier"},{"family":"Schaub","given":"Torsten"}],"citation-key":"fandinnoAnswerSetProgramming2021","issued":{"date-parts":[["2021",11,24]]},"language":"en","number":"arXiv:2111.06366","publisher":"arXiv","source":"arXiv.org","title":"Answer Set Programming Made Easy","type":"article","URL":"http://arxiv.org/abs/2111.06366"},{"id":"fawziDiscoveringFasterMatrix2022","abstract":"Abstract\n            \n              Improving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems—from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero\n              1\n              for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4 × 4 matrices in a finite field, where AlphaTensor’s algorithm improves on Strassen’s two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago\n              2\n              . We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor’s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Fawzi","given":"Alhussein"},{"family":"Balog","given":"Matej"},{"family":"Huang","given":"Aja"},{"family":"Hubert","given":"Thomas"},{"family":"Romera-Paredes","given":"Bernardino"},{"family":"Barekatain","given":"Mohammadamin"},{"family":"Novikov","given":"Alexander"},{"family":"R. Ruiz","given":"Francisco J."},{"family":"Schrittwieser","given":"Julian"},{"family":"Swirszcz","given":"Grzegorz"},{"family":"Silver","given":"David"},{"family":"Hassabis","given":"Demis"},{"family":"Kohli","given":"Pushmeet"}],"citation-key":"fawziDiscoveringFasterMatrix2022","container-title":"Nature","container-title-short":"Nature","DOI":"10.1038/s41586-022-05172-4","ISSN":"0028-0836, 1476-4687","issue":"7930","issued":{"date-parts":[["2022",10,6]]},"language":"en","page":"47-53","source":"DOI.org (Crossref)","title":"Discovering faster matrix multiplication algorithms with reinforcement learning","type":"article-journal","URL":"https://www.nature.com/articles/s41586-022-05172-4","volume":"610"},{"id":"ferraro2005gaussian","author":[{"family":"Ferraro","given":"Alessandro"},{"family":"Olivares","given":"Stefano"},{"family":"Paris","given":"Matteo G. A."}],"citation-key":"ferraro2005gaussian","issued":{"date-parts":[["2005"]]},"title":"Gaussian states in continuous variable quantum information","type":"chapter"},{"id":"gaoRetrievalAugmentedGenerationLarge2024","abstract":"Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and nontransparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces the metrics and benchmarks for assessing RAG models, along with the most up-to-date evaluation framework. In conclusion, the paper delineates prospective avenues for research, including the identification of challenges, the expansion of multi-modalities, and the progression of the RAG infrastructure and its ecosystem. 1.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Gao","given":"Yunfan"},{"family":"Xiong","given":"Yun"},{"family":"Gao","given":"Xinyu"},{"family":"Jia","given":"Kangxiang"},{"family":"Pan","given":"Jinliu"},{"family":"Bi","given":"Yuxi"},{"family":"Dai","given":"Yi"},{"family":"Sun","given":"Jiawei"},{"family":"Wang","given":"Meng"},{"family":"Wang","given":"Haofen"}],"citation-key":"gaoRetrievalAugmentedGenerationLarge2024","issued":{"date-parts":[["2024",3,27]]},"language":"en","number":"arXiv:2312.10997","publisher":"arXiv","source":"arXiv.org","title":"Retrieval-Augmented Generation for Large Language Models: A Survey","title-short":"Retrieval-Augmented Generation for Large Language Models","type":"article","URL":"http://arxiv.org/abs/2312.10997"},{"id":"goldblumNoFreeLunch2023","abstract":"No free lunch theorems for supervised learning state that no learner can solve all problems or that all learners achieve exactly the same accuracy on average over a uniform distribution on learning problems. Accordingly, these theorems are often referenced in support of the notion that individual problems require specially tailored inductive biases. While virtually all uniformly sampled datasets have high complexity, real-world problems disproportionately generate low-complexity data, and we argue that neural network models share this same preference, formalized using Kolmogorov complexity. Notably, we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains. Our experiments show that pre-trained and even randomly initialized language models prefer to generate low-complexity sequences. Whereas no free lunch theorems seemingly indicate that individual problems require specialized learners, we explain how tasks that often require human intervention such as picking an appropriately sized model when labeled data is scarce or plentiful can be automated into a single learning algorithm. These observations justify the trend in deep learning of unifying seemingly disparate problems with an increasingly small set of machine learning models.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Goldblum","given":"Micah"},{"family":"Finzi","given":"Marc"},{"family":"Rowan","given":"Keefer"},{"family":"Wilson","given":"Andrew Gordon"}],"citation-key":"goldblumNoFreeLunch2023","issued":{"date-parts":[["2023",4,11]]},"language":"en","number":"arXiv:2304.05366","publisher":"arXiv","source":"arXiv.org","title":"The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning","type":"article","URL":"http://arxiv.org/abs/2304.05366"},{"id":"gravesNeuralTuringMachines2014","abstract":"We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efﬁciently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Graves","given":"Alex"},{"family":"Wayne","given":"Greg"},{"family":"Danihelka","given":"Ivo"}],"citation-key":"gravesNeuralTuringMachines2014","issued":{"date-parts":[["2014",12,10]]},"language":"en","number":"arXiv:1410.5401","publisher":"arXiv","source":"arXiv.org","title":"Neural Turing Machines","type":"article","URL":"http://arxiv.org/abs/1410.5401"},{"id":"Griffiths_Schroeter_2018","author":[{"family":"Griffiths","given":"David J."},{"family":"Schroeter","given":"Darrell F."}],"citation-key":"Griffiths_Schroeter_2018","edition":"3","event-place":"Cambridge","issued":{"date-parts":[["2018"]]},"publisher":"Cambridge University Press","publisher-place":"Cambridge","title":"Introduction to quantum mechanics","type":"book","URL":"https://ia904607.us.archive.org/3/items/introduction-to-solid-state-physics-by-charles-kittel-urdukutabkhanapk.blogspot.com/Uploaded%20-%2031-03-2021/Physics%20(6)-21-3-2021/Introduction%20to%20Quantum%20Mechanics%20by%20David%20J.%20Griffiths%20And%20Darrell%20F.%20Schroeter%20_(urdukutabkhanapk.blogspot.com).pdf"},{"id":"guanFederatedLearningMedical2024","abstract":"Machine learning in medical imaging often faces a fundamental dilemma, namely, the small sample size problem. Many recent studies suggest using multi-domain data pooled from different acquisition sites/centers to improve statistical power. However, medical images from different sites cannot be easily shared to build large datasets for model training due to privacy protection reasons. As a promising solution, federated learning, which enables collaborative training of machine learning models based on data from different sites without cross-site data sharing, has attracted considerable attention recently. In this paper, we conduct a comprehensive survey of the recent development of federated learning methods in medical image analysis. We have systematically gathered research papers on federated learning and its applications in medical image analysis published between 2017 and 2023. Our search and compilation were conducted using databases from IEEE Xplore, ACM Digital Library, Science Direct, Springer Link, Web of Science, Google Scholar, and PubMed. In this survey, we first introduce the background of federated learning for dealing with privacy protection and collaborative learning issues. We then present a comprehensive review of recent advances in federated learning methods for medical image analysis. Specifically, existing methods are categorized based on three critical aspects of a federated learning system, including client end, server end, and communication techniques. In each category, we summarize the existing federated learning methods according to specific research problems in medical image analysis and also provide insights into the motivations of different approaches. In addition, we provide a review of existing benchmark medical imaging datasets and software platforms for current federated learning research. We also conduct an experimental study to empirically evaluate typical federated learning methods for medical image analysis. This survey can help to better understand the current research status, challenges, and potential research opportunities in this promising research field.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Guan","given":"Hao"},{"family":"Yap","given":"Pew-Thian"},{"family":"Bozoki","given":"Andrea"},{"family":"Liu","given":"Mingxia"}],"citation-key":"guanFederatedLearningMedical2024","container-title":"Pattern Recognition","container-title-short":"Pattern Recognition","DOI":"10.1016/j.patcog.2024.110424","ISSN":"00313203","issued":{"date-parts":[["2024",7]]},"language":"en","page":"110424","source":"DOI.org (Crossref)","title":"Federated learning for medical image analysis: A survey","title-short":"Federated learning for medical image analysis","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S0031320324001754","volume":"151"},{"id":"hanDeepCompressionCompressing2016","abstract":"Neural networks are both computationally intensive and memory intensive, making them difﬁcult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce “deep compression”, a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35× to 49× without affecting their accuracy. Our method ﬁrst prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, ﬁnally, we apply Huffman coding. After the ﬁrst two steps we retrain the network to ﬁne tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9× to 13×; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35×, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49× from 552MB to 11.3MB, again with no loss of accuracy. This allows ﬁtting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3× to 4× layerwise speedup and 3× to 7× better energy efﬁciency.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Han","given":"Song"},{"family":"Mao","given":"Huizi"},{"family":"Dally","given":"William J."}],"citation-key":"hanDeepCompressionCompressing2016","issued":{"date-parts":[["2016",2,15]]},"language":"en","number":"arXiv:1510.00149","publisher":"arXiv","source":"arXiv.org","title":"Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding","title-short":"Deep Compression","type":"article","URL":"http://arxiv.org/abs/1510.00149"},{"id":"haseUnreasonableEffectivenessEasy2024","abstract":"How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current language models often generalize relatively well from easy to hard data, even performing as well as \"oracle\" models trained on hard data. We demonstrate this kind of easy-to-hard generalization using simple training methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect and train on easy data rather than hard data, since hard data is generally noisier and costlier to collect. Our experiments use open models up to 70b in size and four publicly available question-answering datasets with questions ranging in difficulty from 3rd grade science questions to college level STEM questions and general-knowledge trivia. We conclude that easy-to-hard generalization in LMs is surprisingly strong for the tasks studied, suggesting the scalable oversight problem may be easier than previously thought. Our code is available at https://github.com/allenai/easy-to-hard-generalization","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Hase","given":"Peter"},{"family":"Bansal","given":"Mohit"},{"family":"Clark","given":"Peter"},{"family":"Wiegreffe","given":"Sarah"}],"citation-key":"haseUnreasonableEffectivenessEasy2024","issued":{"date-parts":[["2024",1,12]]},"language":"en","number":"arXiv:2401.06751","publisher":"arXiv","source":"arXiv.org","title":"The Unreasonable Effectiveness of Easy Training Data for Hard Tasks","type":"article","URL":"http://arxiv.org/abs/2401.06751"},{"id":"huLoRALowRankAdaptation2021","abstract":"An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full ﬁne-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁnetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Hu","given":"Edward J."},{"family":"Shen","given":"Yelong"},{"family":"Wallis","given":"Phillip"},{"family":"Allen-Zhu","given":"Zeyuan"},{"family":"Li","given":"Yuanzhi"},{"family":"Wang","given":"Shean"},{"family":"Wang","given":"Lu"},{"family":"Chen","given":"Weizhu"}],"citation-key":"huLoRALowRankAdaptation2021","issued":{"date-parts":[["2021",10,16]]},"language":"en","number":"arXiv:2106.09685","publisher":"arXiv","source":"arXiv.org","title":"LoRA: Low-Rank Adaptation of Large Language Models","title-short":"LoRA","type":"article","URL":"http://arxiv.org/abs/2106.09685"},{"id":"huttnerLongrangeQKDTrusted2022","abstract":"A recently published patent (https://www.ipo.gov.uk/p-ipsum/Case/PublicationNumber/GB2590064) has claimed the development of a novel quantum key distribution protocol purporting to achieve long-range quantum security without trusted nodes and without use of quantum repeaters. Here we present a straightforward analysis of this claim, and reach the conclusion that it is largely unfounded.","author":[{"family":"Huttner","given":"Bruno"},{"family":"Alléaume","given":"Romain"},{"family":"Diamanti","given":"Eleni"},{"family":"Fröwis","given":"Florian"},{"family":"Grangier","given":"Philippe"},{"family":"Hübel","given":"Hannes"},{"family":"Martin","given":"Vicente"},{"family":"Poppe","given":"Andreas"},{"family":"Slater","given":"Joshua A."},{"family":"Spiller","given":"Tim"},{"family":"Tittel","given":"Wolfgang"},{"family":"Tranier","given":"Benoit"},{"family":"Wonfor","given":"Adrian"},{"family":"Zbinden","given":"Hugo"}],"citation-key":"huttnerLongrangeQKDTrusted2022","container-title":"npj Quantum Information","container-title-short":"npj Quantum Information","DOI":"10.1038/s41534-022-00613-4","ISSN":"2056-6387","issue":"1","issued":{"date-parts":[["2022",9,9]]},"page":"108","title":"Long-range QKD without trusted nodes is not possible with current technology","type":"article-journal","URL":"https://doi.org/10.1038/s41534-022-00613-4","volume":"8"},{"id":"inestaPerformanceMetricsContinuous2023","accessed":{"date-parts":[["2024",6,6]]},"author":[{"family":"Iñesta","given":"Álvaro G."},{"family":"Wehner","given":"Stephanie"}],"citation-key":"inestaPerformanceMetricsContinuous2023","container-title":"Physical Review A","container-title-short":"Phys. Rev. A","DOI":"10.1103/PhysRevA.108.052615","ISSN":"2469-9926, 2469-9934","issue":"5","issued":{"date-parts":[["2023",11,27]]},"language":"en","page":"052615","source":"DOI.org (Crossref)","title":"Performance metrics for the continuous distribution of entanglement in multiuser quantum networks","type":"article-journal","URL":"https://link.aps.org/doi/10.1103/PhysRevA.108.052615","volume":"108"},{"id":"jainPracticalContinuousvariableQuantum2022","abstract":"A quantum key distribution (QKD) system must fulfill the requirement of universal composability to ensure that any cryptographic application (using the QKD system) is also secure. Furthermore, the theoretical proof responsible for security analysis and key generation should cater to the number N of the distributed quantum states being finite in practice. Continuous-variable (CV) QKD based on coherent states, despite being a suitable candidate for integration in the telecom infrastructure, has so far been unable to demonstrate composability as existing proofs require a rather large N for successful key generation. Here we report a Gaussian-modulated coherent state CVQKD system that is able to overcome these challenges and can generate composable keys secure against collective attacks with N ≈ 2 × 108 coherent states. With this advance, possible due to improvements to the security proof and a fast, yet low-noise and highly stable system operation, CVQKD implementations take a significant step towards their discrete-variable counterparts in practicality, performance, and security.","author":[{"family":"Jain","given":"Nitin"},{"family":"Chin","given":"Hou-Man"},{"family":"Mani","given":"Hossein"},{"family":"Lupo","given":"Cosmo"},{"family":"Nikolic","given":"Dino Solar"},{"family":"Kordts","given":"Arne"},{"family":"Pirandola","given":"Stefano"},{"family":"Pedersen","given":"Thomas Brochmann"},{"family":"Kolb","given":"Matthias"},{"family":"Ömer","given":"Bernhard"},{"family":"Pacher","given":"Christoph"},{"family":"Gehring","given":"Tobias"},{"family":"Andersen","given":"Ulrik L."}],"citation-key":"jainPracticalContinuousvariableQuantum2022","container-title":"Nature Communications","container-title-short":"Nature Communications","DOI":"10.1038/s41467-022-32161-y","ISSN":"2041-1723","issue":"1","issued":{"date-parts":[["2022",8,12]]},"page":"4740","title":"Practical continuous-variable quantum key distribution with composable security","type":"article-journal","URL":"https://doi.org/10.1038/s41467-022-32161-y","volume":"13"},{"id":"jamesKeyManagementSystems2023","abstract":"The Key Management System (KMS) is an important component in scaling up from link-to-link key generation to large key distribution networks. In this work we provide an overview of a KMS in the context of Quantum Key Distribution Networks (QKDN) and give a thorough summary of the functionality of a KMS in such an application. Beyond classical QKDNs, we discuss Post Quantum Cryptography (PQC) hybridization techniques at the KMS level. These methods add an additional layer of security against quantum computer driven attacks. We also discuss selected topics regarding the development, deployment and operation of components for such security infrastructure. In addition, relevant standards in the realm of Quantum Key Distribution (QKD) are outlined and analyzed. As some of the necessary interfaces have not been standardized, namely the interface between two KMS instances and the interface between the KMS and the Software Defined Network (SDN) Agent, we propose APIs for these two cases. The design of the interface between the KMS and QKD modules is discussed and, considering their resource constraints, a push mode for the ETSI GS QKD 004 standard is proposed. Finally, implementation details of a prototype KMS are outlined and trade-offs are discussed.","author":[{"family":"James","given":"Paul"},{"family":"Laschet","given":"Stephan"},{"family":"Ramacher","given":"Sebastian"},{"family":"Torresetti","given":"Luca"}],"citation-key":"jamesKeyManagementSystems2023","collection-title":"ARES '23","container-title":"Proceedings of the 18th International Conference on Availability, Reliability and Security","DOI":"10.1145/3600160.3605050","event-place":"<conf-loc>, <city>Benevento</city>, <country>Italy</country>, </conf-loc>","ISBN":"9798400707728","issued":{"date-parts":[["2023"]]},"publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","title":"Key Management Systems for Large-Scale Quantum Key Distribution Networks","type":"paper-conference","URL":"https://doi.org/10.1145/3600160.3605050"},{"id":"jiangMixtralExperts2024","abstract":"We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model finetuned to follow instructions, Mixtral 8x7B – Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B – chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Jiang","given":"Albert Q."},{"family":"Sablayrolles","given":"Alexandre"},{"family":"Roux","given":"Antoine"},{"family":"Mensch","given":"Arthur"},{"family":"Savary","given":"Blanche"},{"family":"Bamford","given":"Chris"},{"family":"Chaplot","given":"Devendra Singh"},{"family":"Casas","given":"Diego","dropping-particle":"de las"},{"family":"Hanna","given":"Emma Bou"},{"family":"Bressand","given":"Florian"},{"family":"Lengyel","given":"Gianna"},{"family":"Bour","given":"Guillaume"},{"family":"Lample","given":"Guillaume"},{"family":"Lavaud","given":"Lélio Renard"},{"family":"Saulnier","given":"Lucile"},{"family":"Lachaux","given":"Marie-Anne"},{"family":"Stock","given":"Pierre"},{"family":"Subramanian","given":"Sandeep"},{"family":"Yang","given":"Sophia"},{"family":"Antoniak","given":"Szymon"},{"family":"Scao","given":"Teven Le"},{"family":"Gervet","given":"Théophile"},{"family":"Lavril","given":"Thibaut"},{"family":"Wang","given":"Thomas"},{"family":"Lacroix","given":"Timothée"},{"family":"Sayed","given":"William El"}],"citation-key":"jiangMixtralExperts2024","issued":{"date-parts":[["2024",1,8]]},"language":"en","number":"arXiv:2401.04088","publisher":"arXiv","source":"arXiv.org","title":"Mixtral of Experts","type":"article","URL":"http://arxiv.org/abs/2401.04088"},{"id":"jiDifferentialPrivacyMachine2014","abstract":"The objective of machine learning is to extract useful information from data, while privacy is preserved by concealing information. Thus it seems hard to reconcile these competing interests. However, they frequently must be balanced when mining sensitive data. For example, medical research represents an important application where it is necessary both to extract useful information and protect patient privacy. One way to resolve the conﬂict is to extract general characteristics of whole populations without disclosing the private information of individuals.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Ji","given":"Zhanglong"},{"family":"Lipton","given":"Zachary C."},{"family":"Elkan","given":"Charles"}],"citation-key":"jiDifferentialPrivacyMachine2014","issued":{"date-parts":[["2014",12,23]]},"language":"en","number":"arXiv:1412.7584","publisher":"arXiv","source":"arXiv.org","title":"Differential Privacy and Machine Learning: a Survey and Review","title-short":"Differential Privacy and Machine Learning","type":"article","URL":"http://arxiv.org/abs/1412.7584"},{"id":"jiEmergingTrendsFederated2024","abstract":"Federated learning is a new learning paradigm that decouples data collection and model training via multi-party computation and model aggregation. As a flexible learning setting, federated learning has the potential to integrate with other learning frameworks. We conduct a focused survey of federated learning in conjunction with other learning algorithms. Specifically, we explore various learning algorithms to improve the vanilla federated averaging algorithm and review model fusion methods such as adaptive aggregation, regularization, clustered methods, and Bayesian methods. Following the emerging trends, we also discuss federated learning in the intersection with other learning paradigms, termed federated X learning, where X includes multitask learning, meta-learning, transfer learning, unsupervised learning, and reinforcement learning. In addition to reviewing state-of-the-art studies, this paper also identifies key challenges and applications in this field, while also highlighting promising future directions.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Ji","given":"Shaoxiong"},{"family":"Tan","given":"Yue"},{"family":"Saravirta","given":"Teemu"},{"family":"Yang","given":"Zhiqin"},{"family":"Liu","given":"Yixin"},{"family":"Vasankari","given":"Lauri"},{"family":"Pan","given":"Shirui"},{"family":"Long","given":"Guodong"},{"family":"Walid","given":"Anwar"}],"citation-key":"jiEmergingTrendsFederated2024","container-title":"International Journal of Machine Learning and Cybernetics","container-title-short":"Int. J. Mach. Learn. & Cyber.","DOI":"10.1007/s13042-024-02119-1","ISSN":"1868-8071, 1868-808X","issued":{"date-parts":[["2024",4,2]]},"language":"en","source":"DOI.org (Crossref)","title":"Emerging trends in federated learning: from model fusion to federated X learning","title-short":"Emerging trends in federated learning","type":"article-journal","URL":"https://link.springer.com/10.1007/s13042-024-02119-1"},{"id":"kahnGlobalExtremumSeeking2015","abstract":"This paper presents a method for ﬁnding the global maximum of a spatially varying ﬁeld using a multi-agent system. A surrogate model of the ﬁeld is determined via Kriging (Gaussian process regression) from a set of measurements collected by the agents. A criterion exploiting Kriging statistical properties is introduced for selecting new sampling points that each vehicle must rally. These new points are obtained as a compromise between improvement of the estimate of the global maximum and traveling distance. A cooperative control law is proposed to move the agents to the desired sampling points while avoiding collisions. Simulation results show the interest of the method and how it compares with a state-of-the-art solution.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Kahn","given":"Arthur"},{"family":"Marzat","given":"Julien"},{"family":"Piet-Lahanier","given":"Hélène"},{"family":"Kieffer","given":"Michel"}],"citation-key":"kahnGlobalExtremumSeeking2015","container-title":"IFAC-PapersOnLine","container-title-short":"IFAC-PapersOnLine","DOI":"10.1016/j.ifacol.2015.12.182","ISSN":"24058963","issue":"28","issued":{"date-parts":[["2015"]]},"language":"en","license":"https://www.elsevier.com/tdm/userlicense/1.0/","page":"526-531","source":"DOI.org (Crossref)","title":"Global extremum seeking by Kriging with a multi-agent system","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S2405896315028062","volume":"48"},{"id":"katoAdvantageKeyRelay2023","abstract":"The key relay protocol (KRP) plays an important role in improving the performance and the security of quantum key distribution (QKD) networks. On the other hand, there is also an existing research ﬁeld called secure network coding (SNC), which has similar goal and structure. We here analyze differences and similarities between the KRP and SNC rigorously. We found, rather surprisingly, that there is a deﬁnite gap in security between the KRP and SNC; that is, certain KRPs achieve better security than any SNC schemes on the same graph. We also found that this gap can be closed if we generalize the notion of SNC by adding free public channels; that is, KRPs are equivalent to SNC schemes augmented with free public channels.","accessed":{"date-parts":[["2024",5,25]]},"author":[{"family":"Kato","given":"Go"},{"family":"Fujiwara","given":"Mikio"},{"family":"Tsurumaru","given":"Toyohiro"}],"citation-key":"katoAdvantageKeyRelay2023","container-title":"IEEE Transactions on Quantum Engineering","container-title-short":"IEEE Trans. Quantum Eng.","DOI":"10.1109/TQE.2023.3309590","ISSN":"2689-1808","issued":{"date-parts":[["2023"]]},"language":"en","page":"1-17","source":"arXiv.org","title":"Advantage of the key relay protocol over secure network coding","type":"article-journal","URL":"http://arxiv.org/abs/2111.13328","volume":"4"},{"id":"krawec2024finite","author":[{"family":"Krawec","given":"Walter O."},{"family":"Wang","given":"Bing"},{"family":"Brown","given":"Ryan"}],"citation-key":"krawec2024finite","issued":{"date-parts":[["2024"]]},"title":"Finite key security of simplified trusted node networks","type":"article-journal"},{"id":"Laudenbach_2018","author":[{"family":"Laudenbach","given":"Fabian"},{"family":"Pacher","given":"Christoph"},{"family":"Fung","given":"Chi‐Hang Fred"},{"family":"Poppe","given":"Andreas"},{"family":"Peev","given":"Momtchil"},{"family":"Schrenk","given":"Bernhard"},{"family":"Hentschel","given":"Michael"},{"family":"Walther","given":"Philip"},{"family":"Hübel","given":"Hannes"}],"citation-key":"Laudenbach_2018","container-title":"Advanced Quantum Technologies","DOI":"10.1002/qute.201800011","ISSN":"2511-9044","issue":"1","issued":{"date-parts":[["2018",6]]},"publisher":"Wiley","title":"Continuous‐variable quantum key distribution with gaussian modulation—the theory of practical implementations","type":"article-journal","URL":"http://dx.doi.org/10.1002/qute.201800011","volume":"1"},{"id":"lifschitzStronglyEquivalentLogic2001","abstract":"A logic program Π\n              1\n              is said to be equivalent to a logic program Π\n              2\n              in the sense of the answer set semantics if Π\n              1\n              and Π\n              2\n              have the same answer sets. We are interested in the following stronger condition: for every logic program, Π, Π\n              1\n              , ∪ Π has the same answer sets as Π\n              2\n              ∪ Π. The study of strong equivalence is important, because we learn from it how one can simplify a part of a logic program without looking at the rest of it. The main theorem shows that the verification of strong equivalence can be accomplished by cheching the equivalence of formulas in a monotonic logic, called the logic of here-and-there, which is intermediate between classical logic and intuitionistic logic.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Lifschitz","given":"Vladimir"},{"family":"Pearce","given":"David"},{"family":"Valverde","given":"Agustín"}],"citation-key":"lifschitzStronglyEquivalentLogic2001","container-title":"ACM Transactions on Computational Logic","container-title-short":"ACM Trans. Comput. Logic","DOI":"10.1145/383779.383783","ISSN":"1529-3785, 1557-945X","issue":"4","issued":{"date-parts":[["2001",10]]},"language":"en","page":"526-541","source":"DOI.org (Crossref)","title":"Strongly equivalent logic programs","type":"article-journal","URL":"https://dl.acm.org/doi/10.1145/383779.383783","volume":"2"},{"id":"liuVerticalFederatedLearning2024","abstract":"Vertical Federated Learning (VFL) is a federated learning setting where multiple parties with different features about the same set of users jointly train machine learning models without exposing their raw data or model parameters. Motivated by the rapid growth in VFL research and real-world applications, we provide a comprehensive review of the concept and algorithms of VFL, as well as current advances and challenges in various aspects, including effectiveness, efficiency, and privacy. We provide an exhaustive categorization for VFL settings and privacy-preserving protocols and comprehensively analyze the privacy attacks and defense strategies for each protocol. We propose a unified framework, termed VFLow, which considers the VFL problem under communication, computation, privacy, as well as effectiveness and fairness constraints. Finally, we review the most recent advances in industrial applications, highlighting open challenges and future directions for VFL.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Liu","given":"Yang"},{"family":"Kang","given":"Yan"},{"family":"Zou","given":"Tianyuan"},{"family":"Pu","given":"Yanhong"},{"family":"He","given":"Yuanqin"},{"family":"Ye","given":"Xiaozhou"},{"family":"Ouyang","given":"Ye"},{"family":"Zhang","given":"Ya-Qin"},{"family":"Yang","given":"Qiang"}],"citation-key":"liuVerticalFederatedLearning2024","container-title":"IEEE Transactions on Knowledge and Data Engineering","container-title-short":"IEEE Trans. Knowl. Data Eng.","DOI":"10.1109/TKDE.2024.3352628","ISSN":"1041-4347, 1558-2191, 2326-3865","issued":{"date-parts":[["2024"]]},"language":"en","page":"1-20","source":"arXiv.org","title":"Vertical Federated Learning: Concepts, Advances and Challenges","title-short":"Vertical Federated Learning","type":"article-journal","URL":"http://arxiv.org/abs/2211.12814"},{"id":"lundbergUnifiedApproachInterpreting2017","abstract":"Understanding why a model makes a certain prediction can be as crucial as the prediction’s accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a uniﬁed framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identiﬁcation of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class uniﬁes six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this uniﬁcation, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Lundberg","given":"Scott"},{"family":"Lee","given":"Su-In"}],"citation-key":"lundbergUnifiedApproachInterpreting2017","issued":{"date-parts":[["2017",11,24]]},"language":"en","number":"arXiv:1705.07874","publisher":"arXiv","source":"arXiv.org","title":"A Unified Approach to Interpreting Model Predictions","type":"article","URL":"http://arxiv.org/abs/1705.07874"},{"id":"luzonTutorialFederatedLearning2024","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Luzón","given":"M. Victoria"},{"family":"Rodríguez-Barroso","given":"Nuria"},{"family":"Argente-Garrido","given":"Alberto"},{"family":"Jiménez-López","given":"Daniel"},{"family":"Moyano","given":"Jose M."},{"family":"Del Ser","given":"Javier"},{"family":"Ding","given":"Weiping"},{"family":"Herrera","given":"Francisco"}],"citation-key":"luzonTutorialFederatedLearning2024","container-title":"IEEE/CAA Journal of Automatica Sinica","container-title-short":"IEEE/CAA J. Autom. Sinica","DOI":"10.1109/JAS.2024.124215","ISSN":"2329-9266, 2329-9274","issue":"4","issued":{"date-parts":[["2024",4]]},"language":"en","page":"824-850","source":"DOI.org (Crossref)","title":"A Tutorial on Federated Learning from Theory to Practice: Foundations, Software Frameworks, Exemplary Use Cases, and Selected Trends","title-short":"A Tutorial on Federated Learning from Theory to Practice","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/10475194/","volume":"11"},{"id":"mancosuDevelopmentMathematicalLogic2009","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Mancosu","given":"Paolo"},{"family":"Zach","given":"Richard"},{"family":"Badesa","given":"Calixto"}],"citation-key":"mancosuDevelopmentMathematicalLogic2009","container-title":"The Development of Modern Logic","DOI":"10.1093/acprof:oso/9780195137316.003.0029","editor":[{"family":"Haaparanta","given":"Leila"}],"ISBN":"978-0-19-513731-6","issued":{"date-parts":[["2009",6,18]]},"language":"en","page":"318-470","publisher":"Oxford University Press","source":"DOI.org (Crossref)","title":"The Development of Mathematical Logic from Russell to Tarski, 1900–1935","type":"chapter","URL":"https://academic.oup.com/book/25375/chapter/192472884"},{"id":"martinMadQCIHeterogeneousScalable","abstract":"Current quantum key distribution (QKD) networks focus almost exclusively on transporting secret keys with the highest possible rate. Consequently, they are built as mostly fixed, ad hoc, logically, and physically isolated infrastructures designed to avoid any penalty to the quantum channel. This architecture is neither scalable nor cost-effective and future, realworld deployments will differ considerably. The structure of the MadQCI QKD network presented here is based on disaggregated components and modern paradigms especially designed for flexibility, upgradability, and facilitating the integration of QKD in the security and telecommunications-networks ecosystem. These underlying ideas have been tested by deploying many QKD systems from several manufacturers in a real-world, multi-tenant telecommunications network, installed in production facilities and sharing the infrastructure with commercial traffic. Different technologies have been used in different links to address the variety of situations and needs that arise in real networks, exploring a wide range of possibilities. Finally, a set of realistic use cases have been implemented to demonstrate the validity and performance of the network. The testing took place during a period close to three years, where most of the nodes were continuously active.","author":[{"family":"Martin","given":"V"},{"family":"Brito","given":"J P"},{"family":"Ortiz","given":"L"},{"family":"Mendez","given":"R B"},{"family":"Buruaga","given":"J S"},{"family":"Vicente","given":"R J"},{"family":"Sebastián","given":"A"},{"family":"Rincon","given":"D"},{"family":"Perez","given":"F"},{"family":"Sanchez","given":"C"},{"family":"Peev","given":"M"},{"family":"Brunner","given":"H H"},{"family":"Fung","given":"F"},{"family":"Poppe","given":"A"},{"family":"Shields","given":"A J"},{"family":"Woodward","given":"R I"},{"family":"Griesser","given":"H"},{"family":"Roehrich","given":"S"},{"family":"Hentschel","given":"M"},{"family":"Rivas-Moscoso","given":"J M"},{"family":"Pastor","given":"A"},{"family":"Folgueira","given":"J"},{"family":"Lopez","given":"D R"}],"citation-key":"martinMadQCIHeterogeneousScalable","language":"en","source":"Zotero","title":"MadQCI: a heterogeneous and scalable SDN QKD network deployed in production facilities.","type":"article-journal"},{"id":"mccabeMultiplePhysicsPretraining2023","abstract":"We introduce multiple physics pretraining (MPP), an autoregressive task-agnostic pretraining approach for physical surrogate modeling. MPP involves training large surrogate models to predict the dynamics of multiple heterogeneous physical systems simultaneously by learning features that are broadly useful across diverse physical tasks. In order to learn effectively in this setting, we introduce a shared embedding and normalization strategy that projects the fields of multiple systems into a single shared embedding space. We validate the efficacy of our approach on both pretraining and downstream tasks over a broad fluid mechanics-oriented benchmark. We show that a single MPP-pretrained transformer is able to match or outperform task-specific baselines on all pretraining sub-tasks without the need for finetuning. For downstream tasks, we demonstrate that finetuning MPP-trained models results in more accurate predictions across multiple time-steps on new physics compared to training from scratch or finetuning pretrained video foundation models.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"McCabe","given":"Michael"},{"family":"Blancard","given":"Bruno Régaldo-Saint"},{"family":"Parker","given":"Liam Holden"},{"family":"Ohana","given":"Ruben"},{"family":"Cranmer","given":"Miles"},{"family":"Bietti","given":"Alberto"},{"family":"Eickenberg","given":"Michael"},{"family":"Golkar","given":"Siavash"},{"family":"Krawezik","given":"Geraud"},{"family":"Lanusse","given":"Francois"},{"family":"Pettee","given":"Mariel"},{"family":"Tesileanu","given":"Tiberiu"},{"family":"Cho","given":"Kyunghyun"},{"family":"Ho","given":"Shirley"}],"citation-key":"mccabeMultiplePhysicsPretraining2023","issued":{"date-parts":[["2023",10,4]]},"language":"en","number":"arXiv:2310.02994","publisher":"arXiv","source":"arXiv.org","title":"Multiple Physics Pretraining for Physical Surrogate Models","type":"article","URL":"http://arxiv.org/abs/2310.02994"},{"id":"mehicQKDNeworking","abstract":"The convergence of quantum cryptography with applications used in everyday life is a topic drawing attention from the industrial and academic worlds. The development of quantum electronics has led to the practical achievement of quantum devices that are already available on the market and waiting for their first application on a broader scale. A major aspect of quantum cryptography is the methodology of Quantum Key Distribution (QKD), which is used to generate and distribute symmetric cryptographic keys between two geographically separate users using the principles of quantum physics. In previous years, several successful QKD networks have been created to test the implementation and interoperability of different practical solutions. This article surveys previously applied methods, showing techniques for deploying QKD networks and current challenges of QKD networking. Unlike studies focusing on optical channels and optical equipment, this survey focuses on the network aspect by considering network organization, routing and signaling protocols, simulation techniques, and a software-defined QKD networking approach.","author":[{"family":"Mehic","given":"Miralem"},{"family":"Niemiec","given":"Marcin"},{"family":"Rass","given":"Stefan"},{"family":"Ma","given":"Jiajun"},{"family":"Peev","given":"Momtchil"},{"family":"Aguado","given":"Alejandro"},{"family":"Martin","given":"Vicente"},{"family":"Schauer","given":"Stefan"},{"family":"Poppe","given":"Andreas"},{"family":"Pacher","given":"Christoph"},{"family":"Voznak","given":"Miroslav"}],"citation-key":"mehicQKDNeworking","container-title":"Acm Computing Surveys","container-title-short":"ACM Comput. Surv.","DOI":"10.1145/3402192","event-place":"New York, NY, USA","ISSN":"0360-0300","issue":"5","issued":{"date-parts":[["2020",9]]},"number-of-pages":"41","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","title":"Quantum key distribution: a networking perspective","type":"article-journal","URL":"https://doi.org/10.1145/3402192","volume":"53"},{"id":"morrisTOPOLOGYTEARS","author":[{"family":"Morris","given":"Sidney A"}],"citation-key":"morrisTOPOLOGYTEARS","language":"en","source":"Zotero","title":"TOPOLOGY WITHOUT TEARS","type":"book"},{"id":"murrayRandomizedNumericalLinear2023","abstract":"Randomized numerical linear algebra - RandNLA, for short - concerns the use of randomization as a resource to develop improved algorithms for large-scale linear algebra computations. The origins of contemporary RandNLA lay in theoretical computer science, where it blossomed from a simple idea: randomization provides an avenue for computing approximate solutions to linear algebra problems more efficiently than deterministic algorithms. This idea proved fruitful in the development of scalable algorithms for machine learning and statistical data analysis applications. However, RandNLA's true potential only came into focus upon integration with the fields of numerical analysis and \"classical\" numerical linear algebra. Through the efforts of many individuals, randomized algorithms have been developed that provide full control over the accuracy of their solutions and that can be every bit as reliable as algorithms that might be found in libraries such as LAPACK. Recent years have even seen the incorporation of certain RandNLA methods into MATLAB, the NAG Library, NVIDIA's cuSOLVER, and SciKit-Learn. For all its success, we believe that RandNLA has yet to realize its full potential. In particular, we believe the scientific community stands to benefit significantly from suitably defined \"RandBLAS\" and \"RandLAPACK\" libraries, to serve as standards conceptually analogous to BLAS and LAPACK. This 200-page monograph represents a step toward defining such standards. In it, we cover topics spanning basic sketching, least squares and optimization, low-rank approximation, full matrix decompositions, leverage score sampling, and sketching data with tensor product structures (among others). Much of the provided pseudo-code has been tested via publicly available MATLAB and Python implementations.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Murray","given":"Riley"},{"family":"Demmel","given":"James"},{"family":"Mahoney","given":"Michael W."},{"family":"Erichson","given":"N. Benjamin"},{"family":"Melnichenko","given":"Maksim"},{"family":"Malik","given":"Osman Asif"},{"family":"Grigori","given":"Laura"},{"family":"Luszczek","given":"Piotr"},{"family":"Dereziński","given":"Michał"},{"family":"Lopes","given":"Miles E."},{"family":"Liang","given":"Tianyu"},{"family":"Luo","given":"Hengrui"},{"family":"Dongarra","given":"Jack"}],"citation-key":"murrayRandomizedNumericalLinear2023","issued":{"date-parts":[["2023",4,12]]},"language":"en","publisher":"arXiv","source":"arXiv.org","title":"Randomized Numerical Linear Algebra : A Perspective on the Field With an Eye to Software","title-short":"Randomized Numerical Linear Algebra","type":"book","URL":"http://arxiv.org/abs/2302.11474"},{"id":"nielsenQuantumComputationQuantum2010","abstract":"One of the most cited books in physics of all time, Quantum Computation and Quantum Information remains the best textbook in this exciting field of science. This 10th anniversary edition includes an introduction from the authors setting the work in context. This comprehensive textbook describes such remarkable effects as fast quantum algorithms, quantum teleportation, quantum cryptography and quantum error-correction. Quantum mechanics and computer science are introduced before moving on to describe what a quantum computer is, how it can be used to solve problems faster than 'classical' computers and its real-world implementation. It concludes with an in-depth treatment of quantum information. Containing a wealth of figures and exercises, this well-known textbook is ideal for courses on the subject, and will interest beginning graduate students and researchers in physics, computer science, mathematics, and electrical engineering.","archive":"Cambridge Core","author":[{"family":"Nielsen","given":"Michael A."},{"family":"Chuang","given":"Isaac L."}],"citation-key":"nielsenQuantumComputationQuantum2010","DOI":"10.1017/CBO9780511976667","event-place":"Cambridge","ISBN":"978-1-107-00217-3","issued":{"date-parts":[["2010"]]},"publisher":"Cambridge University Press","publisher-place":"Cambridge","source":"Cambridge University Press","title":"Quantum Computation and Quantum Information: 10th Anniversary Edition","type":"book","URL":"https://www.cambridge.org/core/product/01E10196D0A682A6AEFFEA52D53BE9AE"},{"id":"ortegaSQUWALSSzegedyQUantum2024","abstract":"Szegedy’s quantum walk is an algorithm for quantizing a general Markov chain. It has plenty of applications such as many variants of optimizations. In order to check its properties in an errorfree environment, it is important to have a classical simulator. However, the current simulation algorithms require a great deal of memory due to the particular formulation of this quantum walk. In this paper we propose a memory-saving algorithm that scales as O(N 2) with the size N of the graph. We provide additional procedures for simulating Szegedy’s quantum walk over mixed states and also the Semiclassical Szegedy walk. With these techniques we have built a classical simulator in Python called SQUWALS. We show that our simulator scales as O(N 2) in both time and memory resources. This package provides some high-level applications for algorithms based on Szegedy’s quantum walk, as for example the quantum PageRank.","accessed":{"date-parts":[["2024",6,10]]},"author":[{"family":"Ortega","given":"Sergio A."},{"family":"Martin-Delgado","given":"Miguel A."}],"citation-key":"ortegaSQUWALSSzegedyQUantum2024","container-title":"Advanced Quantum Technologies","container-title-short":"Adv Quantum Tech","DOI":"10.1002/qute.202400022","ISSN":"2511-9044, 2511-9044","issued":{"date-parts":[["2024",4,14]]},"language":"en","page":"2400022","source":"arXiv.org","title":"SQUWALS: A Szegedy QUantum WALks Simulator","title-short":"SQUWALS","type":"article-journal","URL":"http://arxiv.org/abs/2307.14314"},{"id":"pabloREDISCOVERINGNEWTONGRAVITY","abstract":"We present an approach for using machine learning to automatically discover a physical law and associated properties of the system from real observations. We trained a neural network-based architecture, whose structure corresponds to clas- sical mechanics, to simulate the dynamics of our Solar System from 30 years of observed trajectory data. We then used symbolic regression to extract a symbolic formula for the force law, which our results show matches Newtonian gravity. We find that by scaling the model’s predicted acceleration by a trainable scalar variable, we could infer bodies’ (relative) masses despite that they were not ob- servable in the data itself. Though “Newtonian” gravity has of course been known since Newton, our approach did not require knowledge of this physical law, and so our results serve as a proof of principle that our method can extract unknown laws from observed data. This work takes a step towards using modern machine learning tools beyond data processing and analysis, but automated scientific the- ory formation and development.","author":[{"family":"Pablo","given":"Lemos"},{"family":"Jeffrey","given":"Niall"},{"family":"Cranmer","given":"Miles"},{"family":"Battaglia","given":"Peter"},{"family":"Ho","given":"Shirley"}],"citation-key":"pabloREDISCOVERINGNEWTONGRAVITY","title":"REDISCOVERING NEWTON’S GRAVITY AND SOLAR SYSTEM PROPERTIES USING DEEP LEARNING AND IN- DUCTIVE BIASES","type":"article-journal","URL":"https://simdl.github.io/files/59.pdf"},{"id":"pearceEquilibriumLogic2006","abstract":"Equilibrium logic is a general purpose nonmonotonic reasoning formalism closely aligned with answer set programming (ASP). In particular it provides a logical foundation for ASP as well as an extension of the basic syntax of answer set programs. We present an overview of equilibrium logic and its main properties and uses.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Pearce","given":"David"}],"citation-key":"pearceEquilibriumLogic2006","container-title":"Annals of Mathematics and Artificial Intelligence","container-title-short":"Ann Math Artif Intell","DOI":"10.1007/s10472-006-9028-z","ISSN":"1012-2443, 1573-7470","issue":"1-2","issued":{"date-parts":[["2006",11,9]]},"language":"en","license":"http://www.springer.com/tdm","page":"3-41","source":"DOI.org (Crossref)","title":"Equilibrium logic","type":"article-journal","URL":"http://link.springer.com/10.1007/s10472-006-9028-z","volume":"47"},{"id":"PhysRevA.47.R747","author":[{"family":"Eberhard","given":"Philippe H."}],"citation-key":"PhysRevA.47.R747","container-title":"Physical Review A: Atomic, Molecular, and Optical Physics","container-title-short":"Phys. Rev. A","DOI":"10.1103/PhysRevA.47.R747","issue":"2","issued":{"date-parts":[["1993",2]]},"number-of-pages":"0","page":"R747–R750","publisher":"American Physical Society","title":"Background level and counter efficiencies required for a loophole-free Einstein-Podolsky-Rosen experiment","type":"article-journal","URL":"https://link.aps.org/doi/10.1103/PhysRevA.47.R747","volume":"47"},{"id":"PhysRevA.91.012338","author":[{"family":"Stacey","given":"William"},{"family":"Annabestani","given":"Razieh"},{"family":"Ma","given":"Xiongfeng"},{"family":"Lütkenhaus","given":"Norbert"}],"citation-key":"PhysRevA.91.012338","container-title":"Physical Review A: Atomic, Molecular, and Optical Physics","container-title-short":"Phys. Rev. A","DOI":"10.1103/PhysRevA.91.012338","issue":"1","issued":{"date-parts":[["2015",1]]},"number-of-pages":"10","page":"012338","publisher":"American Physical Society","title":"Security of quantum key distribution using a simplified trusted relay","type":"article-journal","URL":"https://link.aps.org/doi/10.1103/PhysRevA.91.012338","volume":"91"},{"id":"PhysRevD.2.1418","author":[{"family":"Pearle","given":"Philip M."}],"citation-key":"PhysRevD.2.1418","container-title":"Physical Review D: Particles and Fields","container-title-short":"Phys. Rev. D","DOI":"10.1103/PhysRevD.2.1418","issue":"8","issued":{"date-parts":[["1970",10]]},"number-of-pages":"0","page":"1418–1425","publisher":"American Physical Society","title":"Hidden-variable example based upon data rejection","type":"article-journal","URL":"https://link.aps.org/doi/10.1103/PhysRevD.2.1418","volume":"2"},{"id":"PhysRevLett.105.070501","author":[{"family":"Gisin","given":"Nicolas"},{"family":"Pironio","given":"Stefano"},{"family":"Sangouard","given":"Nicolas"}],"citation-key":"PhysRevLett.105.070501","container-title":"Physical Review Letters","container-title-short":"Phys. Rev. Lett.","DOI":"10.1103/PhysRevLett.105.070501","issue":"7","issued":{"date-parts":[["2010",8]]},"number-of-pages":"4","page":"070501","publisher":"American Physical Society","title":"Proposal for implementing device-independent quantum key distribution based on a heralded qubit amplifier","type":"article-journal","URL":"https://link.aps.org/doi/10.1103/PhysRevLett.105.070501","volume":"105"},{"id":"PhysRevLett.81.5932","author":[{"family":"Briegel","given":"H.-J."},{"family":"Dür","given":"W."},{"family":"Cirac","given":"J. I."},{"family":"Zoller","given":"P."}],"citation-key":"PhysRevLett.81.5932","container-title":"Physical Review Letters","container-title-short":"Phys. Rev. Lett.","DOI":"10.1103/PhysRevLett.81.5932","issue":"26","issued":{"date-parts":[["1998",12]]},"number-of-pages":"0","page":"5932–5935","publisher":"American Physical Society","title":"Quantum repeaters: The role of imperfect local operations in quantum communication","type":"article-journal","URL":"https://link.aps.org/doi/10.1103/PhysRevLett.81.5932","volume":"81"},{"id":"QSNPRoadmapCryptography","abstract":"First Deriverable for the task 6 of QSNP European Project.","citation-key":"QSNPRoadmapCryptography","title":"QSNP D.6.1 Roadmap on Cryptography","type":"document"},{"id":"Qureshi_2022","abstract":"Quantum state teleportation is an important protocol that plays a pivotal role in various quantum information tasks. Here we theoretically investigate quantum state teleportation by exploiting a general two-mode Gaussian entangled state produced by a parametric converter when two single-mode Gaussian states (SMGSs) in terms of the non-classicality and purity are employed as inputs to the parametric converter. In particular, the time-dependent teleportation fidelity is analyzed with respect to the squeezing parameter and phase-space quadratures of the teleported squeezed coherent state. We show that the teleportation fidelity is maximal when ratio of the expectation values of photon number in the two modes of the evolved Gaussian entangled state is equal to 1. Quantum state teleportation in terms of the purity and non-classicality of the general two SMGSs seems to be a good choice for experimental realization of quantum communication and information processing.","author":[{"family":"Qureshi","given":"Haleema Sadia"},{"family":"Ullah","given":"Shakir"},{"family":"Ghafoor","given":"Fazal"}],"citation-key":"Qureshi_2022","container-title":"Journal of Physics B: Atomic, Molecular and Optical Physics","DOI":"10.1088/1361-6455/ac7370","issue":"14","issued":{"date-parts":[["2022",6]]},"page":"145501","publisher":"IOP Publishing","title":"Time-dependent quantum teleportation via a parametric converter","type":"article-journal","URL":"https://dx.doi.org/10.1088/1361-6455/ac7370","volume":"55"},{"id":"rafailovDirectPreferenceOptimization2023","abstract":"While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Rafailov","given":"Rafael"},{"family":"Sharma","given":"Archit"},{"family":"Mitchell","given":"Eric"},{"family":"Ermon","given":"Stefano"},{"family":"Manning","given":"Christopher D."},{"family":"Finn","given":"Chelsea"}],"citation-key":"rafailovDirectPreferenceOptimization2023","issued":{"date-parts":[["2023",12,13]]},"language":"en","number":"arXiv:2305.18290","publisher":"arXiv","source":"arXiv.org","title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model","title-short":"Direct Preference Optimization","type":"article","URL":"http://arxiv.org/abs/2305.18290"},{"id":"romera-paredesMathematicalDiscoveriesProgram2024","abstract":"Abstract\n            \n              Large language models (LLMs) have demonstrated tremendous capabilities in solving complex tasks, from quantitative reasoning to understanding natural language. However, LLMs sometimes suffer from confabulations (or hallucinations), which can result in them making plausible but incorrect statements\n              1,2\n              . This hinders the use of current large models in scientific discovery. Here we introduce FunSearch (short for searching in the function space), an evolutionary procedure based on pairing a pretrained LLM with a systematic evaluator. We demonstrate the effectiveness of this approach to surpass the best-known results in important problems, pushing the boundary of existing LLM-based approaches\n              3\n              . Applying FunSearch to a central problem in extremal combinatorics—the cap set problem—we discover new constructions of large cap sets going beyond the best-known ones, both in finite dimensional and asymptotic cases. This shows that it is possible to make discoveries for established open problems using LLMs. We showcase the generality of FunSearch by applying it to an algorithmic problem, online bin packing, finding new heuristics that improve on widely used baselines. In contrast to most computer search approaches, FunSearch searches for programs that describe how to solve a problem, rather than what the solution is. Beyond being an effective and scalable strategy, discovered programs tend to be more interpretable than raw solutions, enabling feedback loops between domain experts and FunSearch, and the deployment of such programs in real-world applications.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Romera-Paredes","given":"Bernardino"},{"family":"Barekatain","given":"Mohammadamin"},{"family":"Novikov","given":"Alexander"},{"family":"Balog","given":"Matej"},{"family":"Kumar","given":"M. Pawan"},{"family":"Dupont","given":"Emilien"},{"family":"Ruiz","given":"Francisco J. R."},{"family":"Ellenberg","given":"Jordan S."},{"family":"Wang","given":"Pengming"},{"family":"Fawzi","given":"Omar"},{"family":"Kohli","given":"Pushmeet"},{"family":"Fawzi","given":"Alhussein"}],"citation-key":"romera-paredesMathematicalDiscoveriesProgram2024","container-title":"Nature","container-title-short":"Nature","DOI":"10.1038/s41586-023-06924-6","ISSN":"0028-0836, 1476-4687","issue":"7995","issued":{"date-parts":[["2024",1,18]]},"language":"en","page":"468-475","source":"DOI.org (Crossref)","title":"Mathematical discoveries from program search with large language models","type":"article-journal","URL":"https://www.nature.com/articles/s41586-023-06924-6","volume":"625"},{"id":"rotmaninstituteofphilosophyMarissaGiustinaSignificant2016","abstract":"Local realism is the worldview in which physical properties of objects exist independently of measurement and where physical influences cannot travel faster than the speed of light. Bell’s theorem states that this worldview is incompatible with the predictions of quantum mechanics, as is expressed in Bell’s inequalities. Previous experiments convincingly supported the quantum predictions. Yet, every experiment requires assumptions that provide loopholes for a local realist explanation. In this paper, I will discuss the recent results from my laboratory, in which we designed an experiment that closes the most significant of these loopholes simultaneously. Using a well-optimized source of entangled photons, rapid setting generation, and highly efficient superconducting detectors, we observe a violation of a Bell inequality with high statistical significance. The purely statistical probability of our results to occur under local realism is exceedingly unlikely, corresponding to an 11.5 standard deviation effect.\n\nMarissa Giustina University of Vienna, Physics\nInformation-Theoretic Interpretations of Quantum Mechanics: 2016 Annual Philosophy of Physics Conference\nJune 11-12, 2016\n\nVisit http://philphysics.uwo.ca for conference details.\n\nVisit the Rotman website for more information on applications, events, project descriptions and openings. http://www.rotman.uwo.ca\n\nFollow The Rotman Institute on Twitter:   / rotmanphilo  \n\nLike The Rotman Institute on Facebook:   / rotmaninstitute  \n\nSubscribe to our channel:    / rotmanphilosophy","accessed":{"date-parts":[["2024",5,22]]},"citation-key":"rotmaninstituteofphilosophyMarissaGiustinaSignificant2016","dimensions":"1:16:36","director":[{"literal":"Rotman Institute of Philosophy"}],"issued":{"date-parts":[["2016",7,5]]},"source":"YouTube","title":"Marissa Giustina: Significant loophole-free test of Bell’s theorem with entangled photons","title-short":"Marissa Giustina","type":"motion_picture","URL":"https://www.youtube.com/watch?v=tgoWM4Jcl-s"},{"id":"salvail2009security","author":[{"family":"Salvail","given":"Louis"},{"family":"Peev","given":"Momtchil"},{"family":"Diamanti","given":"Eleni"},{"family":"Alleaume","given":"Romain"},{"family":"Lutkenhaus","given":"Norbert"},{"family":"Laenger","given":"Thomas"}],"citation-key":"salvail2009security","issued":{"date-parts":[["2009"]]},"title":"Security of trusted repeater quantum key distribution networks","type":"article-journal"},{"id":"Scherer:11","abstract":"We develop a model for practical, entanglement-based long-distance quantum key distribution employing entanglement swapping as a key building block. Relying only on existing off-the-shelf technology, we show how to optimize resources so as to maximize secret key distribution rates. The tools comprise lossy transmission links, such as telecom optical fibers or free space, parametric down-conversion sources of entangled photon pairs, and threshold detectors that are inefficient and have dark counts. Our analysis provides the optimal trade-off between detector efficiency and dark counts, which are usually competing, as well as the optimal source brightness that maximizes the secret key rate for specified distances (i.e. loss) between sender and receiver.","author":[{"family":"Scherer","given":"Artur"},{"family":"Sanders","given":"Barry C."},{"family":"Tittel","given":"Wolfgang"}],"citation-key":"Scherer:11","container-title":"Optics Express","container-title-short":"Opt. Express","DOI":"10.1364/OE.19.003004","issue":"4","issued":{"date-parts":[["2011",2]]},"page":"3004–3018","publisher":"Optica Publishing Group","title":"Long-distance practical quantum key distribution by entanglement swapping","type":"article-journal","URL":"https://opg.optica.org/oe/abstract.cfm?URI=oe-19-4-3004","volume":"19"},{"id":"schiavonHeraldedSinglePhoton2016","abstract":"Single photon sources (SPS) are a fundamental building block for optical implementations of quantum information protocols. Among SPSs, multiple crystal heralded single photon sources seem to give the best compromise between high pair production rate and low multiple photon events. In this work, we study their performance in a practical quantum key distribution experiment, by evaluating the achievable key rates. The analysis focuses on the two different schemes, symmetric and asymmetric, proposed for the practical implementation of heralded single photon sources, with attention on the performance of their composing elements. The analysis is based on the protocol proposed by Bennett and Brassard in 1984 and on its improvement exploiting decoy state technique. Finally, a simple way of exploiting the post-selection mechanism for a passive, one decoy state scheme is evaluated.","accessed":{"date-parts":[["2024",5,22]]},"author":[{"family":"Schiavon","given":"M."},{"family":"Vallone","given":"G."},{"family":"Ticozzi","given":"F."},{"family":"Villoresi","given":"P."}],"citation-key":"schiavonHeraldedSinglePhoton2016","container-title":"Physical Review A","container-title-short":"Phys. Rev. A","DOI":"10.1103/PhysRevA.93.012331","ISSN":"2469-9926, 2469-9934","issue":"1","issued":{"date-parts":[["2016",1,20]]},"language":"en","page":"012331","source":"arXiv.org","title":"Heralded single photon sources for QKD applications","type":"article-journal","URL":"http://arxiv.org/abs/1512.01020","volume":"93"},{"id":"scottaaronsonIntroductionQuantumInformation2018","author":[{"family":"Scott Aaronson","given":""}],"citation-key":"scottaaronsonIntroductionQuantumInformation2018","issued":{"season":3,"date-parts":[[2018]]},"language":"en","number-of-pages":"260","number-of-volumes":"1","title":"Introduction to Quantum Information Science Lecture Notes","type":"book","URL":"https://www.scottaaronson.com/qclec.pdf","volume":"1"},{"id":"tomamichelTightFinitekeyAnalysis2012","abstract":"Despite enormous theoretical and experimental progress in quantum cryptography, the security of most current implementations of quantum key distribution is still not rigorously established. One significant problem is that the security of the final key strongly depends on the number, M, of signals exchanged between the legitimate parties. Yet, existing security proofs are often only valid asymptotically, for unrealistically large values of M. Another challenge is that most security proofs are very sensitive to small differences between the physical devices used by the protocol and the theoretical model used to describe them. Here we show that these gaps between theory and experiment can be simultaneously overcome by using a recently developed proof technique based on the uncertainty relation for smooth entropies.","author":[{"family":"Tomamichel","given":"Marco"},{"family":"Lim","given":"Charles Ci Wen"},{"family":"Gisin","given":"Nicolas"},{"family":"Renner","given":"Renato"}],"citation-key":"tomamichelTightFinitekeyAnalysis2012","container-title":"Nature Communications","container-title-short":"Nature Communications","DOI":"10.1038/ncomms1631","ISSN":"2041-1723","issue":"1","issued":{"date-parts":[["2012",1,17]]},"page":"634","title":"Tight finite-key analysis for quantum cryptography","type":"article-journal","URL":"https://doi.org/10.1038/ncomms1631","volume":"3"},{"id":"trinhSolvingOlympiadGeometry2024","abstract":"Abstract\n            \n              Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning\n              1–4\n              , owing to their reputed difficulty among the world’s best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges\n              1,5\n              , resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Trinh","given":"Trieu H."},{"family":"Wu","given":"Yuhuai"},{"family":"Le","given":"Quoc V."},{"family":"He","given":"He"},{"family":"Luong","given":"Thang"}],"citation-key":"trinhSolvingOlympiadGeometry2024","container-title":"Nature","container-title-short":"Nature","DOI":"10.1038/s41586-023-06747-5","ISSN":"0028-0836, 1476-4687","issue":"7995","issued":{"date-parts":[["2024",1,18]]},"language":"en","page":"476-482","source":"DOI.org (Crossref)","title":"Solving olympiad geometry without human demonstrations","type":"article-journal","URL":"https://www.nature.com/articles/s41586-023-06747-5","volume":"625"},{"id":"unknown","author":[{"family":"Kashyap","given":"Ravi"}],"citation-key":"unknown","issued":{"date-parts":[["2022",12]]},"title":"Quantum key distribution: Ekert E91 protocol derivations","type":"document"},{"id":"varmaExplainingGrokkingCircuit2023","abstract":"One of the most surprising puzzles in neural network generalisation is grokking: a network with perfect training accuracy but poor generalisation will, upon further training, transition to perfect generalisation. We propose that grokking occurs when the task admits a generalising solution and a memorising solution, where the generalising solution is slower to learn but more efficient, producing larger logits with the same parameter norm. We hypothesise that memorising circuits become more inefficient with larger training datasets while generalising circuits do not, suggesting there is a critical dataset size at which memorisation and generalisation are equally efficient. We make and confirm four novel predictions about grokking, providing significant evidence in favour of our explanation. Most strikingly, we demonstrate two novel and surprising behaviours: ungrokking, in which a network regresses from perfect to low test accuracy, and semi-grokking, in which a network shows delayed generalisation to partial rather than perfect test accuracy.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Varma","given":"Vikrant"},{"family":"Shah","given":"Rohin"},{"family":"Kenton","given":"Zachary"},{"family":"Kramár","given":"János"},{"family":"Kumar","given":"Ramana"}],"citation-key":"varmaExplainingGrokkingCircuit2023","issued":{"date-parts":[["2023",9,5]]},"language":"en","number":"arXiv:2309.02390","publisher":"arXiv","source":"arXiv.org","title":"Explaining grokking through circuit efficiency","type":"article","URL":"http://arxiv.org/abs/2309.02390"},{"id":"velickovicGraphAttentionNetworks2018","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods’ features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Veličković","given":"Petar"},{"family":"Cucurull","given":"Guillem"},{"family":"Casanova","given":"Arantxa"},{"family":"Romero","given":"Adriana"},{"family":"Liò","given":"Pietro"},{"family":"Bengio","given":"Yoshua"}],"citation-key":"velickovicGraphAttentionNetworks2018","issued":{"date-parts":[["2018",2,4]]},"language":"en","number":"arXiv:1710.10903","publisher":"arXiv","source":"arXiv.org","title":"Graph Attention Networks","type":"article","URL":"http://arxiv.org/abs/1710.10903"},{"id":"vyasRelaxingTrustAssumptions2024","abstract":"Quantum security over long distances with untrusted relays is largely unfounded and is still an open question for active research. Nevertheless, quantum networks based on trusted relays are being built across the globe. However, standard QKD network architecture implores a complete trust requirement on QKD relays, which is too demanding and limits the use cases for QKD networks. In this work, we explore the possibility to securely relay a secret in a QKD network by relaxing the trust assumptions (if not completely) on the relay. We characterize QKD relays with different trust levels, namely, Full Access Trust (FAT), Partial Access Trust (PAT), and No Access Trust (NAT). As the name suggests, each level defines the degree with which a relay is required to be trusted with the secret provided by the key management system for endto-end communication. We then review and propose multiple constructions of the QKD key management system based on the different trust levels. Main contribution of the paper is realized by evaluating key management systems with no access trust level. In principle, we review key management with centralized topology and propose a new decentralized key management system. These different topologies provide various advantages based on the QKD network requirements, allowing an operational flexibility in the architecture. We believe this work presents a new perspective to the open problem of providing a confiding and a practical solution for future long range secure communications.","accessed":{"date-parts":[["2024",6,9]]},"author":[{"family":"Vyas","given":"Nilesh"},{"family":"Mendes","given":"Paulo"}],"citation-key":"vyasRelaxingTrustAssumptions2024","issued":{"date-parts":[["2024",2,20]]},"language":"en","number":"arXiv:2402.13136","publisher":"arXiv","source":"arXiv.org","title":"Relaxing Trust Assumptions on Quantum Key Distribution Networks","type":"article","URL":"http://arxiv.org/abs/2402.13136"},{"id":"wang2010hidden","author":[{"family":"Wang","given":"Frédéric"}],"citation-key":"wang2010hidden","issued":{"date-parts":[["2010"]]},"title":"The hidden subgroup problem","type":"thesis"},{"id":"wernerAllMultipartiteBell2001","abstract":"We construct a set of 2^(2^n) independent Bell correlation inequalities for n-partite systems with two dichotomic observables each, which is complete in the sense that the inequalities are satisfied if and only if the correlations considered allow a local classical model. All these inequalities can be summarized in a single, albeit non-linear inequality. We show that quantum correlations satisfy this condition provided the state has positive partial transpose with respect to any grouping of the n systems into two subsystems. We also provide an efficient algorithm for finding the maximal quantum mechanical violation of each inequality, and show that the maximum is always attained for the generalized GHZ state.","accessed":{"date-parts":[["2024",5,22]]},"author":[{"family":"Werner","given":"R. F."},{"family":"Wolf","given":"M. M."}],"citation-key":"wernerAllMultipartiteBell2001","container-title":"Physical Review A","container-title-short":"Phys. Rev. A","DOI":"10.1103/PhysRevA.64.032112","ISSN":"1050-2947, 1094-1622","issue":"3","issued":{"date-parts":[["2001",8,17]]},"language":"en","page":"032112","source":"arXiv.org","title":"All multipartite Bell correlation inequalities for two dichotomic observables per site","type":"article-journal","URL":"http://arxiv.org/abs/quant-ph/0102024","volume":"64"},{"id":"woltersStudyFewShotAudio2020","abstract":"Advances in deep learning have resulted in stateof-the-art performance for many audio classiﬁcation tasks but, unlike humans, these systems traditionally require large amounts of data to make accurate predictions. Not every person or organization has access to those resources, and the organizations that do, like our ﬁeld at large, do not reﬂect the demographics of our country. Enabling people to use machine learning without signiﬁcant resource hurdles is important, because machine learning is an increasingly useful tool for solving problems, and can solve a broader set of problems when put in the hands of a broader set of people. Few-shot learning is a type of machine learning designed to enable the model to generalize to new classes with very few examples. In this research, we address two audio classiﬁcation tasks (speaker identiﬁcation and activity classiﬁcation) with the Prototypical Network few-shot learning algorithm, and assess performance of various encoder architectures. Our encoders include recurrent neural networks, as well as one- and two-dimensional convolutional neural networks. We evaluate our model for speaker identiﬁcation on the VoxCeleb dataset and ICSI Meeting Corpus, obtaining 5-shot 5-way accuracies of 93.5% and 54.0%, respectively. We also evaluate for activity classiﬁcation from audio using few-shot subsets of the Kinetics 600 dataset and AudioSet, both drawn from Youtube videos, obtaining 51.5% and 35.2% accuracy, respectively.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Wolters","given":"Piper"},{"family":"Careaga","given":"Chris"},{"family":"Hutchinson","given":"Brian"},{"family":"Phillips","given":"Lauren"}],"citation-key":"woltersStudyFewShotAudio2020","issued":{"date-parts":[["2020",12,2]]},"language":"en","number":"arXiv:2012.01573","publisher":"arXiv","source":"arXiv.org","title":"A Study of Few-Shot Audio Classification","type":"article","URL":"http://arxiv.org/abs/2012.01573"},{"id":"wuSelectedSolutionsRudin","author":[{"family":"Wu","given":"Wentao"}],"citation-key":"wuSelectedSolutionsRudin","language":"en","source":"Zotero","title":"Selected Solutions to Rudin’s “Principles of Mathematical Analysis”","type":"article-journal"},{"id":"yangSSLNetSynergisticSpectral2024","abstract":"Efficient and accurate bird sound classification is of important for ecology, habitat protection and scientific research, as it plays a central role in monitoring the distribution and abundance of species. However, prevailing methods typically demand extensively labeled audio datasets and have highly customized frameworks, imposing substantial computational and annotation loads. In this study, we present an efficient and general framework called SSL-Net, which combines spectral and learned features to identify different bird sounds. Encouraging empirical results gleaned from a standard field-collected bird audio dataset validate the efficacy of our method in extracting features efficiently and achieving heightened performance in bird sound classification, even when working with limited sample sizes. Furthermore, we present three feature fusion strategies, aiding engineers and researchers in their selection through quantitative analysis.","accessed":{"date-parts":[["2024",5,19]]},"author":[{"family":"Yang","given":"Yiyuan"},{"family":"Zhou","given":"Kaichen"},{"family":"Trigoni","given":"Niki"},{"family":"Markham","given":"Andrew"}],"citation-key":"yangSSLNetSynergisticSpectral2024","container-title":"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","DOI":"10.1109/ICASSP48485.2024.10445889","event-place":"Seoul, Korea, Republic of","event-title":"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","ISBN":"9798350344851","issued":{"date-parts":[["2024",4,14]]},"language":"en","license":"https://doi.org/10.15223/policy-029","page":"926-930","publisher":"IEEE","publisher-place":"Seoul, Korea, Republic of","source":"DOI.org (Crossref)","title":"SSL-Net: A Synergistic Spectral and Learning-Based Network for Efficient Bird Sound Classification","title-short":"SSL-Net","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10445889/"},{"id":"zapateroLongdistanceDeviceindependentQuantum2019","abstract":"Besides being a beautiful idea, device-independent quantum key distribution (DIQKD) is probably the ultimate solution to defeat quantum hacking. Its security is based on a loophole-free violation of a Bell inequality, which results in a very limited maximum achievable distance. To overcome this limitation, DIQKD must be furnished with heralding devices like, for instance, qubit amplifiers, which can signal the arrival of a photon before the measurement settings are actually selected. In this way, one can decouple channel loss from the selection of the measurement settings and, consequently, it is possible to safely post-select the heralded events and discard the rest, which results in a significant enhancement of the achievable distance. In this work, we investigate photonic-based DIQKD assisted by two main types of qubit amplifiers in the finite data block size scenario, and study the resources—particularly, the detection efficiency of the photodetectors and the quality of the entanglement sources—that would be necessary to achieve long-distance DIQKD within a reasonable time frame of signal transmission.","author":[{"family":"Zapatero","given":"Víctor"},{"family":"Curty","given":"Marcos"}],"citation-key":"zapateroLongdistanceDeviceindependentQuantum2019","container-title":"Scientific Reports","container-title-short":"Scientific Reports","DOI":"10.1038/s41598-019-53803-0","ISSN":"2045-2322","issue":"1","issued":{"date-parts":[["2019",11,28]]},"page":"17749","title":"Long-distance device-independent quantum key distribution","type":"article-journal","URL":"https://doi.org/10.1038/s41598-019-53803-0","volume":"9"},{"id":"stoudenmireSupervisedLearningTensor2016a","type":"paper-conference","abstract":"Tensor networks are approximations of high-order tensors which are efficient to work with and have been very successful for physics and mathematics applications. We demonstrate how algorithms for optimizing tensor networks can be adapted to supervised learning tasks by using matrix product states (tensor trains) to parameterize non-linear kernel learning models. For the MNIST data set we obtain less than 1% test set classification error. We discuss an interpretation of the additional structure imparted by the tensor network to the learned model.","container-title":"Advances in Neural Information Processing Systems","publisher":"Curran Associates, Inc.","source":"Neural Information Processing Systems","title":"Supervised Learning with Tensor Networks","URL":"https://proceedings.neurips.cc/paper_files/paper/2016/hash/5314b9674c86e3f9d1ba25ef9bb32895-Abstract.html","volume":"29","author":[{"family":"Stoudenmire","given":"Edwin"},{"family":"Schwab","given":"David J"}],"accessed":{"date-parts":[["2024",6,13]]},"issued":{"date-parts":[["2016"]]},"citation-key":"stoudenmireSupervisedLearningTensor2016a","library":"madq-uam-upm-bib","citekey":"stoudenmireSupervisedLearningTensor2016a"},{"id":"biamonteTensorNetworksNutshell2017","type":"article","abstract":"Tensor network methods are taking a central role in modern quantum physics and beyond. They can provide an efficient approximation to certain classes of quantum states, and the associated graphical language makes it easy to describe and pictorially reason about quantum circuits, channels, protocols, open systems and more. Our goal is to explain tensor networks and some associated methods as quickly and as painlessly as possible. Beginning with the key definitions, the graphical tensor network language is presented through examples. We then provide an introduction to matrix product states. We conclude the tutorial with tensor contractions evaluating combinatorial counting problems. The first one counts the number of solutions for Boolean formulae, whereas the second is Penrose's tensor contraction algorithm, returning the number of $3$-edge-colorings of $3$-regular planar graphs.","language":"en","note":"arXiv:1708.00006 [cond-mat, physics:gr-qc, physics:hep-th, physics:math-ph, physics:quant-ph]","number":"arXiv:1708.00006","publisher":"arXiv","source":"arXiv.org","title":"Tensor Networks in a Nutshell","URL":"http://arxiv.org/abs/1708.00006","author":[{"family":"Biamonte","given":"Jacob"},{"family":"Bergholm","given":"Ville"}],"accessed":{"date-parts":[["2024",6,13]]},"issued":{"date-parts":[["2017",7,31]]},"citation-key":"biamonteTensorNetworksNutshell2017","library":"My Library","citekey":"biamonteTensorNetworksNutshell2017"},{"id":"mnihAsynchronousMethodsDeep2016","type":"article","abstract":"We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.","language":"en","note":"arXiv:1602.01783 [cs]","number":"arXiv:1602.01783","publisher":"arXiv","source":"arXiv.org","title":"Asynchronous Methods for Deep Reinforcement Learning","URL":"http://arxiv.org/abs/1602.01783","author":[{"family":"Mnih","given":"Volodymyr"},{"family":"Badia","given":"Adrià Puigdomènech"},{"family":"Mirza","given":"Mehdi"},{"family":"Graves","given":"Alex"},{"family":"Lillicrap","given":"Timothy P."},{"family":"Harley","given":"Tim"},{"family":"Silver","given":"David"},{"family":"Kavukcuoglu","given":"Koray"}],"accessed":{"date-parts":[["2024",6,25]]},"issued":{"date-parts":[["2016",6,16]]},"citation-key":"mnihAsynchronousMethodsDeep2016","library":"My Library","citekey":"mnihAsynchronousMethodsDeep2016"}]